# Genetic Deep Learning
This project aims to use genetic algorithms to boost the learning of DNN.  Building and training a family  of NN with same structure and hyperparameters from scratch but starting from different random weights.   After a few epochs, mate the trained weights together in order to produce the next generation.  

Main problems to solve when NN:

- Architecture optimization:finding optimal layers and number of nodes in each layer of the network required to capture features from given data.
- Hyperparameter optimization: refers to choosing values of hyperparameters like - learning rate, optimization algorithm, dropout rate, batch size, etc. 

- Weight optimization: fiend the right values for each neuron within each weight in order to solve the gereal equation with a minimum error.

This projects focus on using Genetic Algorithms combined with Gradient Descent to create a novel method of solving the weight optimization problem.
